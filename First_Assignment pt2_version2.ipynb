{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Assignment\n",
    "\n",
    "Each correct answer is worth 1 point. Each wrong answer is worth 0 points.\n",
    "\n",
    "**Deadline: To Be Confirmed by the Tutor**. Please upload this .ipynb file to the submission point (Assignments) in Canvas along with your declaration of authorship."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question A\n",
    "\n",
    "Of the following statements, which are true and which are false?\n",
    "\n",
    "#### 1. Anomaly detection is an example of supervised learning.\n",
    "\n",
    "* **False**, Anomaly detection is a type of unsupervised learning. It is finding anomalies (ex. outliers) on data that don't have a specific label and are seem suspicious or have rare occurrences\n",
    "\n",
    "#### 2. Regression is a type of supervised learning\n",
    "* **True**, In regression the input/label belongs to a continuous range (you are trying to predict the output of a function). As Professor Mazzi states in lecture 1.2 *\"Regression is supervised learning on a continuous label on a continuous function\"* \n",
    "\n",
    "#### 3. Self-supervised learning and semi-supervised learning are synonyms\n",
    "* **False**, Semi-supervised and self-supervised learning are not related and are two different things because semi-supervised learning is a type of supervised learning where a small portion of the dataset have labels (remember, unsupervised learning models do take in labeled data). On the other hand, self-supervised learning is a type of unsupervised learning since it does not take in labeled training data. Instead, self-supervised learning attempts to solve tasks that are completed by supervised learning models (such as getting a list of sentences and getting the self-supervised model to predict the missing word without using are pre-trained/labeled data). \n",
    "\n",
    "#### 4. Regularisation is a technique used to tune the hyperparameters of a machine learning model\n",
    "* **False**, Regularisation and hyperparameter tuning are two separate concepts. Regularisation is used to reduce/eliminate model overfitting. Overfitting occurs when you have a complex model which would perform really well on your training data (such as making predictions or categorising the data), but would not perform well when used on the validation or test data. Regularisation fixes model overfitting by making the model a straight line (linear model). Hyperparameter tuning is when you want to change a machine learning models hyperparameters (aka hardcoded parameters) which can't be learned during the training phase of the model. The goal of hyperparameters is to minimize the loss function output. Both, regularisation and hyperparameter tuning are different concepts but they have a common goal which is to improve the machine learning model accuracy.\n",
    "\n",
    "####  5. Most of the machine learning algorithms require the data to be scaled.\n",
    "* **True**, Most machine learning algorithms do not perform well if the numerical attributes of your input data do not have proper scales. It is important to perform feature scaling on the numerical attributes of the dataset to ensure that all features have the same scale. This ensures that the machine learning model is trained accurately.  \n",
    "\n",
    "*You can edit this cell to fill in you answers*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question B\n",
    "\n",
    "Consider the following snippet of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_circles\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = make_circles(n_samples=1_000, factor=0.3, noise=0.05, random_state=0)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of the following statements, which are true and which are false?\n",
    "\n",
    "#### 1. The test set contains 33% of the samples of the training set\n",
    "* **False**, the `train_test_split()` function is supposed to split the training dataset into training and testing data. In the example code the `test_size` or `train_size` parameter/argument was not defined which means that the `train_test_split()` function will not assign 33% of samples (in the original dataset) as test data. The `train_test_split()` function will automatically allocate 25% of the samples (in the original dataset) as test data.\n",
    "* Also, the question implies that the test dataset will contain the same samples that are in the training dataset which is not true. The training and test dataset are split at the very begininng of ML model training because we want to use the test dataset in the end (when the model is fully trained) to see how well the model responds to new/unseen data. So it would be counterintuitive to day that the test dataset would contain a certain percentage of samples in the training data set when the whole purpose of creating the two datasets is to have one dataset that contains totally 'new' data that the ML model has not seen.\n",
    "\n",
    "####  2. None of the records in the training set has missing values\n",
    "* **True**, In the code below I checked the number of `NaN` or missing values in the `X_train` and `y_train` arrays. I created a list which stored the count of the times a `NaN` value occurs. For both arrays, the count of `NaN` values was zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The numbers of missing values in the \u001b[1mx_train\u001b[0m array is: 0\n",
      "The numbers of missing values in the \u001b[1my_train\u001b[0m array is: 0\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "\n",
    "### creating a list which contains the number of NA or missing values in the `X_train` array\n",
    "x_training_list = [] \n",
    "\n",
    "#the `numpy.isnan()` function converts the `X_train` array into a array of true and false values which show if any value has a NaN/missing value\n",
    "for x in numpy.isnan(X_train): #going through each row in the `X_train` array of trues and false values which indicate if there are any NaN/null values in the array\n",
    "    x_training_list.append(op.countOf(x , True)) #using `op.countOf()` to count the number of true values in each list in the array\n",
    "\n",
    "print('The numbers of missing values in the \\033[1mx_train\\033[0m array is:', numpy.unique(x_training_list)[0]) #the count of true values is zero meaning there are no missing values\n",
    "\n",
    "### creating a list which contains the number of NA or missing values in the `y_train` array\n",
    "y_training_list = [] \n",
    "for y in numpy.isnan(y_train):\n",
    "    y_training_list.append(y)\n",
    "\n",
    "#note: I can directly count the number of NaN/missing values in the output since the `y_training_list` output is a single list of trues and falses\n",
    "print('The numbers of missing values in the \\033[1my_train\\033[0m array is:', y_training_list.count(True)) #since the `y_train` array did not include lists within the array I could directly count the values of trues and falses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. `make_circles` generates a dataset\n",
    "* **True**, the `train_test_split()` function is used to create training and testing data and to do that it needs the original dataset. `X` and `y` were used to create the training data which means that these variables are the original dataset. These variables were created by the `make_circles()` function which means that `make_circles()` generates a dataset. Additionaly, these is also documentation on the skikit learn website which talks about they type of data that `make_circles()` produces.\n",
    "\n",
    "####  4. The resulting dataset contains two features\n",
    "* **True**, The shape of the input data `X` is (1000, 2) which means the data contains two features (representing the x and y coordinates). The shape of the target data `y` is (1000, ) since this data contains information about whether the data point belongs to the inner or outer circle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 2)\n",
      "(1000,)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape) #x in the input variable that describes the dataset features\n",
    "print(y.shape) #y is our target data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  5. The resulting dataset can be used for multi-class classification\n",
    "* **False**, The target data `y` only contains 0 and 1 values so binary classification should be used for this dataset. The code below prints out the unique values in the target data\n",
    "\n",
    "\n",
    "*You can edit this cell to fill in you answers*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of unique values in the y target dataset: [0, 1]\n"
     ]
    }
   ],
   "source": [
    "print('List of unique values in the y target dataset:', list(set(y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question C\n",
    "\n",
    "Consider the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SGDRegressor()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SGDRegressor</label><div class=\"sk-toggleable__content\"><pre>SGDRegressor()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SGDRegressor()"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "X, y = make_regression(n_samples=200, n_features=2, noise=0.2)\n",
    "\n",
    "sgd_reg = SGDRegressor(\n",
    "    loss=\"squared_error\",\n",
    "    max_iter=1000,\n",
    ")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    train_size=0.8,\n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "sgd_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of the following statements, which are true and which are false?\n",
    "\n",
    "####  1. The regressor `sgd_reg` has learned three parameters\n",
    "* **True**, we have two independent variables defined (from `n_features=2`) and then we also have one output variable `y`. These are the three parameters that the multiple linear regression model will learn when it is being trained.\n",
    "* In terms of the `sgd_reg` function there are only 2 parameters/attributes that have been specified in the `sgd_reg()` regressor which is the loss function (`loss=\"squared_error\"`) and number of epochs that will go over the training data (`max_iter=1000`)\n",
    "\n",
    "####  2. The regressor is trained on 180 data instances\n",
    "* **False**, in the `make_regression()` function we assigned 200 samples to our original dataset and in the `train_test_split()` function we assigned 80% of the original data as training data through the `train_size=0.8` parameter which means that 160 data instances are in the training dataset, not 180. For the regressor to train 180 data instances the `train_size` parameter needs to be set to `0.9`\n",
    "\n",
    "####  3. `sgd_reg` is trained to fit a logistic regressor\n",
    "* **False**, `sgd_reg` is trained to fit linear regression using stochastic gradient descent (SGD). Additionally, the mean square error loss function is used to perform the regression training and this loss function is used for linear regression tasks (this is because the graph of the mean square error loss function is a parabola which allows gradient decent to be performed. Due to the non-linearity of logistic regression, gradient decent can't be used to minimize the loss of a function since there will be many 'lows' or 'parabolas' in the loss function graph. \n",
    "\n",
    "####  4. The initial learning rate is a hyperparameter of `sgd_reg`\n",
    "* **True**, The learning rate of a regressor is a hyperparameter since this can't be learned/changed automatically during model training. This value needs to be defined manually. The `eta0 =` parameter can define the learning rate of the `sgd_reg()` regressor, since the learning rate was not defined in the code the default learning rate is 0.01 \n",
    "\n",
    "####  5. `sgd_reg` is a linear model\n",
    "* **True**, the `sgd_reg()` regressor fits a linear model to the dataset by finding the minimal loss value in the loss function (the minimum loss value is found through stochastic gradient descent -SGD)\n",
    "\n",
    "*You can edit this cell to fill in you answers*"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4adc19297ff67e92365d65fbb293b9763c71c243813490754e9fef78d503e715"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
