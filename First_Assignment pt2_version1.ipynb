{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Assignment\n",
    "\n",
    "Each correct answer is worth 1 point. Each wrong answer is worth 0 points.\n",
    "\n",
    "**Deadline: To Be Confirmed by the Tutor**. Please upload this .ipynb file to the submission point (Assignments) in Canvas along with your declaration of authorship."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question A\n",
    "\n",
    "Of the following statements, which are true and which are false?\n",
    "\n",
    "#### 1. Anomaly detection is an example of supervised learning.\n",
    "* **False**, Anomaly detection is an example of unsupervised learning as mentioned in [Lecture 1.2: Intro to ML -min. 15:47](https://ox.cloud.panopto.eu/Panopto/Pages/Viewer.aspx?id=fc2bd5eb-c1cf-4596-a96f-ae8600b723c7&start=0). Anomaly detection is used to detect anomalies (basically any data points that fall out of the normal range of data. These data points are known as outliers or noise)\n",
    "\n",
    "#### 2. Regression is a type of supervised learning\n",
    "* **True**, The two types of supervised learning are regression and classification -reference [Lecture 1.2: Intro to ML -min. 12:20](https://ox.cloud.panopto.eu/Panopto/Pages/Viewer.aspx?id=fc2bd5eb-c1cf-4596-a96f-ae8600b723c7&start=0)\n",
    "\n",
    "#### 3. Self-supervised learning and semi-supervised learning are synonyms\n",
    "* **False**, when describing the differences between semi-supervised and self-supervised learning Professor Izzo describes self-supervised learning as *\"quite a different beast\"* when compared with semi-supervised learning -reference [Lecture 1.2: Intro to ML -min. 20:33](https://ox.cloud.panopto.eu/Panopto/Pages/Viewer.aspx?id=fc2bd5eb-c1cf-4596-a96f-ae8600b723c7&start=0). The reason for this is mainly due to the fact that semi-supervised learning takes in training/input data (which aligns it more with supervised learning), but the training data is not fully labeled (there is a small sample of the training data that is labeled). Pseudo labelling techniques will need to be used to fill out the remaining un-labeled rows in the training data. On the other hand, self-supervised learning does not take in training data (which connects more with un-supervised learning). Self-supervised learning does try to solve tasks that are completed by semi-supervised learning models (such as predicting missing words in a sentence or prediciting a sequence of numbers without giving any previous training data to the semi-supervised learning model)\n",
    "\n",
    "####  4. Regularisation is a technique used to tune the hyperparameters of a machine learning model\n",
    "* **False**, In [Lecture 1.3: Effectiveness of Data, Overfitting and Training/Test sets](https://ox.cloud.panopto.eu/Panopto/Pages/Viewer.aspx?id=13597731-8d11-45f2-b684-ae8600b7236b&start=0) Professor Izzo talks about regularisation and hyperparameter tuning. In [min. 2:39](https://ox.cloud.panopto.eu/Panopto/Pages/Viewer.aspx?id=13597731-8d11-45f2-b684-ae8600b7236b&start=0) Professor Izzo explaines that regularisation reduces model overfitting since *\"models affected by overfitting do not generalize well on all datasets, they only perform extremely well on the training dataset\"*. The goal of regularisation is to allow the ML to also perform correct predictions on the validation and testing data. In [min. 8:59](https://ox.cloud.panopto.eu/Panopto/Pages/Viewer.aspx?id=13597731-8d11-45f2-b684-ae8600b7236b&start=0) Professor Izzo explaines that hyperparameter tuning is used to change the hyperparameter (aka hardcoded parameters) of machine learning models, that *\"cannot be learning during the training phase\"*.\n",
    "* Additionally, in [Lecture 3.3: Polynomial Regression. Cross-validation. Regularization](https://ox.cloud.panopto.eu/Panopto/Pages/Viewer.aspx?id=6efd45de-3cd9-4b9a-befe-ae97007f9fad&start=0) Professor Izzo applies polynomial regression on the \"Kings County House Data\". In [min. 9:15](https://ox.cloud.panopto.eu/Panopto/Pages/Viewer.aspx?id=6efd45de-3cd9-4b9a-befe-ae97007f9fad&start=0) Professor Izzo mentions how we need to *\"rely on regularisation to avoid overfitting/underfitting\"* and can then *\"apply the model in the validation dataset\"*. Professor Izzo makes the distinction that regularisation is different from hyperparameter tuning in [min. 9:38](https://ox.cloud.panopto.eu/Panopto/Pages/Viewer.aspx?id=6efd45de-3cd9-4b9a-befe-ae97007f9fad&start=0) when he states that *\"the testing dataset cannot be used until we have tweaked all of the model hyperparameters\"*. So regularisation is used before we apply the model on the validation data and hyperparameter tuning is needed before we apply the model on the testing data. While regularisation can prevent overfitting by changing the model parameters (ex. weights), hyperparameter tuning focuses on changing the models hyperparameters/hardcoded parameters (ex. number of nodes and layers in a neural network, learning rate) which need to be changed manually and can't be learned automatically.\n",
    "\n",
    "#### 5. Most of the machine learning algorithms require the data to be scaled.\n",
    "* **True**, In [Lecture 3.1: Pre-processing data pipeline: imputation, feature engineering, and scaling](https://ox.cloud.panopto.eu/Panopto/Pages/Viewer.aspx?id=9c07c095-8b52-45fd-bfad-ae97007fa057&start=0) Professor Izzo talks about the importance of feature scaling and why the numerical attributes in the input data needs to all have a common scale. In [min. 15:25](https://ox.cloud.panopto.eu/Panopto/Pages/Viewer.aspx?id=9c07c095-8b52-45fd-bfad-ae97007fa057&start=0) Professor Izzo states *\"in majority of the cases machine learning algorithms do not perform well if your input numerical outputs have very different scales\"*. Professor Izzo then shows various boxplot visualisations which visualise the feature scaling of the numerical variables in the Kings County dataset. The viualisations illustrate how the scales of some numerical features/attributes are completely different than others.\n",
    "\n",
    "*You can edit this cell to fill in you answers*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question B\n",
    "\n",
    "Consider the following snippet of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_circles\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = make_circles(n_samples=1_000, factor=0.3, noise=0.05, random_state=0)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    y, \n",
    "                                                    stratify=y, \n",
    "                                                    random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of the following statements, which are true and which are false?\n",
    "\n",
    "#### 1. The test set contains 33% of the samples of the training set\n",
    "* **False**, firstly, the question states that the test dataset *\"contains 33% of the samples of the training set\"*. This statement is incorrent because the testing and training dataset are two ccompletely different entities that serve completely different purposes (although the objective of both datasets is to train the machine learning model). In [lecture 1.3: Effectiveness of Data, Overfitting and Training/Test sets](https://ox.cloud.panopto.eu/Panopto/Pages/Viewer.aspx?id=13597731-8d11-45f2-b684-ae8600b7236b&start=0) at [min. 7:19](https://ox.cloud.panopto.eu/Panopto/Pages/Viewer.aspx?id=13597731-8d11-45f2-b684-ae8600b7236b&start=0) Professor Izzo states how the training and test datasets need to be split and how the testing data *\"must not be used to train the algorithm at all, you must keep it away untill you have chosen the final model\"*. So based of what Professor Izzo has explained the testing dataset can never consist of samples of the training dataset, the test dataset can only consist of samples in the original dataset (the dataset before being split into testing and training data)\n",
    "* Secondly, the `test_size` or `train_size` parameters are not defined in the `train_test_split()` function which means that we can't say that the testing dataset contains 33% of the samples in the original dataset. By default, the `train_test_split()` function would assign 25% of the samples (in the original dataset) to the testing dataset. [-reference (containes documentataion of the `train_test_split()` function](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)\n",
    "\n",
    "#### 2. None of the records in the training set has missing values\n",
    "* **True**, I created two lists which stored `True` and `False` boolean values of whether the `X_train` and `y_train` training dataset arrays had any `NaN` missing values. By using for loops, `append()`, and `sum()` function I was able to count the number of `True` values in each array (this would tell me if the training dataset had any `NaN` values. The count of `True` values that I got for `X_train` and `y_train` was zero which meant that none of the records in the training set had any missing values.\n",
    "* A second approach that I used was to search what the `make_circles` function does from the `sklearn`. I was curious to see what type of data does the `make_circles` function create and understand how a missing value could potentially be created using this function. Based on the [scikit learn documention on the `make_circles` function](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_circles.html#:~:text=Make%20a%20large%20circle%20containing,visualize%20clustering%20and%20classification%20algorithms.) I read that the `make_circles` function makes a *\"large circle containing a smaller circle in 2d\"* and this dataset is used to *\"o visualize clustering and classification algorith\"*. Since the purpose of this function was to help individuals learn and grasp the concepts of clustering and classification I made a hypothesis that this dataset would not include any missing values since it is used for learning. To test this hypothesis I decided to create my own small test dataset of 5 samples instead of 1,000 (my test data is labeled as `sample_X` and `sample_y`. When I printed the outputs I saw that my dataset of the inner and outer circle was created without any missing values. This means that the training datasets will not contain any missing values since the original dataset does not have any missing values to begin with.  s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The X_train array has 0 NaN or missing values.\n",
      "The y_train array has 0 NaN or missing values.\n"
     ]
    }
   ],
   "source": [
    "############################ \n",
    "####### APPROACH 1 #########\n",
    "############################\n",
    "import numpy as np\n",
    "import operator as op\n",
    "\n",
    "X_train_nan_list = [] #creating a list which will store the counts of NaN or missing values in the `X_train` training dataset array\n",
    "y_train_nan_list = [] #creating a list which will store the counts of NaN or missing values in the `y_train` training dataset array\n",
    "\n",
    "############################ appending X_train_nan_list\n",
    "#I am using a for loop which goes over each iteration of the values in the `X_train` array\n",
    "#I use the `np.isnan()` function to convert the array values to a list of true and false values to indicate if there are any NaN values\n",
    "#I then count the sum of each list (in the array). Python counts true values as 1 so the `sum()` function will let me know if there are any NaN values\n",
    "for i in np.isnan(X_train): X_train_nan_list.append(sum(i))\n",
    "X_train_nan_list = set(X_train_nan_list) #getting the list of unique values of the counts of trues and falses in the `X_train_nan_list` list\n",
    "print('The X_train array has', list(X_train_nan_list)[0], 'NaN or missing values.')\n",
    "\n",
    "############################ y_train_nan_list\n",
    "for i in np.isnan(y_train): y_train_nan_list.append(i)\n",
    "print('The y_train array has', sum(y_train_nan_list), 'NaN or missing values.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.3489369   0.11204466]\n",
      " [ 1.0933779  -0.04886389]\n",
      " [-0.95249558 -0.00756786]\n",
      " [-0.15516094  0.28033755]\n",
      " [-0.14279782 -0.18709395]]\n",
      "[1 0 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "############################ \n",
    "####### APPROACH 2 #########\n",
    "############################\n",
    "sample_X, sample_y = make_circles(n_samples=5, factor=0.3, noise=0.05, random_state=0)\n",
    "\n",
    "#the make_circles() function was able to produce an output for my small sample without any missing values which means that the training data will not include missing values as well\n",
    "print(sample_X)\n",
    "print(sample_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####   3. `make_circles` generates a dataset\n",
    "* **True**, Please refer to my response in the previous question (Question B.2). I was able to show how the `make_circles()` function generates the original dataset. This dataset is also used to visualise clusturing and classification algorithms. [- link to skikit learn documentation on make_circles](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_circles.html#:~:text=Make%20a%20large%20circle%20containing,visualize%20clustering%20and%20classification%20algorithms.)\n",
    "\n",
    "#### 4. The resulting dataset contains two features\n",
    "* **True**, if you look at the sample circle dataset that I created in question 2 (where there are 5 samples in the `sample_X` and `sample_y` dataset) you can see that there are two features in the input data `sample_X` which represent the x and y coordinates. The target data `sample_y` includes one feature since it categorizes each data point in the inner or outer circle (0 means the data point is in the inner circle and 1 means that the data point is in the outer circle, I got this information [from the skikit learn documentation on the `make_circle()` function](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_circles.html#:~:text=Make%20a%20large%20circle%20containing,visualize%20clustering%20and%20classification%20algorithms.).\n",
    "* I verified these observations by checking the shapes of the original input and target attribute `X` and `y`. The shape of `X` was (1000, 2) which means that the input data has two features. The shape of `y` was (1000, 1) which meant that target data has one feature. I have shown my code below:\n",
    "* *personal note*: target attributes are only chosen when we are dealing with a supervised learning task. Since the `make_circles()` function is used to demonstrate classification concepts (ex. classifying each data point in a inner and outer circle) it makes sense why a target dataset is being created (-reference [min. 6:04 in 2.2: Data Exploration and Visualization](https://ox.cloud.panopto.eu/Panopto/Pages/Viewer.aspx?id=d1943120-3a52-4e38-b3eb-ae8d00b2a65a&start=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the input data X is: (1000, 2)\n",
      "The shape of the target data y is: (1000,)\n"
     ]
    }
   ],
   "source": [
    "print('The shape of the input data X is:', X.shape)\n",
    "print('The shape of the target data y is:', y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. The resulting dataset can be used for multi-class classification\n",
    "* **False**, At min. 1:09 in lecture [4.1: Classification Theory, Logistic and Softmax Regression, Support Vector Machines](https://ox.cloud.panopto.eu/Panopto/Pages/Viewer.aspx?id=54615b10-7503-4bfe-9ef6-ae9b010655da&start=0) Professor Izzo says that *\"binary classification is the simplist type of classification task where you only have to predict labels that belong to two classes\"* (ex. yes-no good-bad true-false 1-0 results). The target dataset `y` only includes 0 and 1 groups for classification (I explained how 0 and 1 are used to categorize the data points in the previous questions and I have printed the unique values of the target dataset below). Professor Izzo then goes on to explain how multi-class classification is only used *\"when you have more than one label to predict\"*. Our target dataset does not have more than two labels to predict which is why multi-class classification would not be an appropriate classification type to use for model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1]\n"
     ]
    }
   ],
   "source": [
    "y_unique_values_list = list(set(y))\n",
    "print(y_unique_values_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question C\n",
    "\n",
    "Consider the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SGDRegressor()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SGDRegressor</label><div class=\"sk-toggleable__content\"><pre>SGDRegressor()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SGDRegressor()"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "X, y = make_regression(n_samples=200, n_features=2, noise=0.2)\n",
    "sgd_reg = SGDRegressor(\n",
    "    loss=\"squared_error\",\n",
    "    max_iter=1000,\n",
    ")\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    train_size=0.8,\n",
    "    random_state=0\n",
    ")\n",
    "sgd_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of the following statements, which are true and which are false?\n",
    "\n",
    "####  1. The regressor `sgd_reg` has learned three parameters\n",
    "* **True**, we have defined two independent variables in the `make_regression()` function by defining two features. We also have a dependent variable `y` which the model will need to predict the value of so in total, the regressor has 3 parameters that it has to learn. So the equation will be $y = \\beta_0 + \\beta_1 * x_1 + \\beta_2 * x_2 + \\varepsilon_i $  where the three parameters are the slope ($\\beta_1$ and $\\beta_2$), y-intercept ($\\beta_0$) -reference [min. 31:00 on lecture 1.4: Introduction to Linear Regression](https://ox.cloud.panopto.eu/Panopto/Pages/Viewer.aspx?id=17e91b68-ba76-4e18-a1d3-b10400a21c77&start=1843.529057)\n",
    "\n",
    "####  2. The regressor is trained on 180 data instances\n",
    "* **False**, the `n_samples=` attribute defines the number of samples that will be used for regression (it is the original dataset) -[link to documentation on the `make_regression()` function](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html). In this example, we have assigned 200 samples to our original dataset and have specified that 80% of the data needs to be training data (this is specified in the `train_test_split()` function with the `train_size()` attribute. 80% of 200 is 160 which means that 160 samples will be in the training data instead of 180 data samples. \n",
    "\n",
    "####  3. `sgd_reg` is trained to fit a logistic regressor\n",
    "* **False**, `sgd_reg` uses stochastic gradient descent (SGD) to perform linear regression, it is not used to fit a logistic regression. In [min. 15:08 on lecture 3.2: Linear Regression. Gradient Descent](https://ox.cloud.panopto.eu/Panopto/Pages/Viewer.aspx?id=6c3067ce-4a8b-456c-8db2-ae97007f9fea&start=0) Professor Izzo explaines the charts of how different gradient decents can over/underestimate the ML model and how *\"there can be more complicated shaped loss functions with many minimums\"* and using gradient decent on these functions can give you a local mimimum instead of a global minimum which would not be a optimal solution. The loss function graph of a logistic regression model is [non-convex](https://medium.com/@hemaanushatangellamudi/loss-minimization-interpretation-of-logistic-regression-49515d7e62cb) meaning it has many minimums.\n",
    "\n",
    "####  4. The initial learning rate is a hyperparameter of `sgd_reg`\n",
    "* **True**, the `eta0=` attribute/hyperparameter specifies the learning rate of the regressor and this is a hyperparameter since the learning rate is something that needs to be defined manualy and can't be 'learned' by the model ([here is a link to the documentation of the `sgd_reg` regressor](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDRegressor.html). Also, Proffsor Izzo uses the `eta0=` hyperparameter to specify the learning rate in [min. 26:47 on Lecture 3.2: Linear Regression. Gradient Descent](https://ox.cloud.panopto.eu/Panopto/Pages/Viewer.aspx?id=6c3067ce-4a8b-456c-8db2-ae97007f9fea&start=0)\n",
    "\n",
    "####  5. `sgd_reg` is a linear model\n",
    "* **True**, `sgd_reg` uses stochastic gradient descent (SGD) to perform linear regression on the data points/samples. Also, stochastic gradient descent (SGD) is only used on linear models since is can only provide an optimal solution for convex loss function graphs. The `sgd_reg` regressor uses the mean squared error loss function which has a convex graph (I explain this concept further in question 3).\n",
    "\n",
    "*You can edit this cell to fill in you answers*"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4adc19297ff67e92365d65fbb293b9763c71c243813490754e9fef78d503e715"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
